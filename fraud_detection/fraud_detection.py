# -*- coding: utf-8 -*-
"""fraud_detection.ipynb
dataset : https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/versions/3?resource=download
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p7Wbz84yaMEDaAUMogD8yq3pi48j0jym

# Fraud Detection

Fraud adalah tindakan curang yang dilakukan sedemikian rupa, sehingga menguntungkan diri sendiri, kelompok, atau pihak lain (perorangan, perusahaan atau institusi). Terjadinya suatu fraud disebabkan oleh beberapa alasan dan faktor yang mempengaruhinya. Di dalam dunia pembayaran, transaksi fraud adalah transaksi yang tidak sah atau ilegal. Misalnya adalah transaksi kartu kredit yang tidak diketahui oleh pemilik kartu. Mereka yang melakukan transaksi fraud tersebut biasanya disebut fraudsters, dan menggunakan informasi kartu untuk membeli barang/jasa untuk kepentingan dirinya.

## Deteksi penipuan menggunakan data yang tidak berlabel
"""

import matplotlib.pyplot as plt

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd

# %matplotlib inline

df = pd.read_csv('data/banksim.csv')
df.head()

df.shape

df.groupby('category').mean()

"""kita dapat melihat dari rata-rata kategori bahwa penipuan lebih banyak terjadi di kategori ini

### segmentasi kostumer
"""

df.groupby('age').mean()

df['age'].value_counts()

"""kita dapat melihat jumlah rata-rata yang dibelanjakan serta terjadinya penipuan agak mirip di seluruh kelompok. Kelompok usia '0' menonjol tetapi karena hanya ada 40 kasus, tidak masuk akal untuk membaginya dalam kelompok terpisah dan menjalankan model terpisah pada mereka.

### menggunakan statistik untuk mendefinisikan normal fraud

contoh ini memberi Kita gambaran tentang bagaimana transaksi penipuan berbeda secara struktural dari transaksi normal.
"""

df_fraud = df.loc[df.fraud == 1] 
df_non_fraud = df.loc[df.fraud == 0]

plt.hist(df_fraud.amount, alpha=0.5, label='fraud')
plt.hist(df_non_fraud.amount, alpha=0.5, label='nonfraud')
plt.legend();

"""karena jumlah pengamatan penipuan jauh lebih kecil, sulit untuk melihat distribusi penuh. Meskipun demikian, kita dapat melihat bahwa transaksi penipuan cenderung berada di sisi yang lebih besar dibandingkan dengan pengamatan normal. Ini kabar baik, karena membantu kita nantinya dalam mendeteksi fraud dari non-fraud.

### Scaling data
"""

df.drop('Unnamed: 0', axis=1, inplace=True)

df.head()

df.gender.value_counts()

df = df.drop(df[(df.gender == "E") | (df.gender == "U")].index)

df.gender.unique()

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
gender = le.fit_transform(df.gender)

df['M'] = pd.get_dummies(df.gender)['M']

df = pd.concat([df,pd.get_dummies(df.category, drop_first=True)],axis=1)

y = df.fraud.copy()

df.drop(['gender', 'fraud', 'category'], axis=1, inplace=True)

df.head()

df.shape

y.shape

from sklearn.preprocessing import MinMaxScaler

X = np.array(df).astype(np.float)

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

X_scaled.shape

"""### menggunakan metode ``K-means clustering``

K-Means Clustering adalah suatu metode penganalisaan data atau metode Data Mining yang melakukan proses pemodelan unssupervised learning dan menggunakan metode yang mengelompokan data berbagai partisi.

K Means Clustering memiliki objective yaitu meminimalisasi object function yang telah di atur pada proses clasterisasi. Dengan cara minimalisasi variasi antar 1 cluster dengan maksimalisasi variasi dengan data di cluster lainnya.

K means clustering merupakan metode algoritma dasar,yang diterapkan sebagai berikut

- Menentukan jumlah cluster
- Secara acak mendistribusikan data cluster
- Menghitung rata rata dari data yang ada di cluster.
- Menggunakan langkah baris 3 kembali sesuai nilai treshold
- Menghitung jarak antara data dan nilai centroid(K means clustering)
- Distance space dapat diimplementasikan untuk menghitung jarak data dan centroid. Contoh penghitungan jarak yang sering digunakan adalah manhattan/city blok distance
"""

from sklearn.cluster import MiniBatchKMeans

kmeans = MiniBatchKMeans(n_clusters=8, random_state=0)
kmeans.fit(X_scaled)

"""### menggunakan metode ``Elbow``

menggunakan metode elbow untuk optimisasi K pada kmeans
"""

clustno = range(1, 10)

kmeans = [MiniBatchKMeans(n_clusters=i) for i in clustno] 
score = [kmeans[i].fit(X_scaled).score(X_scaled) for i in range(len(kmeans))]

plt.plot(clustno, score)
plt.xlabel('Number of Clusters')
plt.ylabel('Score')
plt.title('Elbow Curve');

"""sekarang kita dapat melihat bahwa jumlah cluster yang optimal mungkin harus berada di sekitar 3 cluster, karena di situlah siku berada di kurva.

### deteksi outliers
"""

from sklearn.model_selection import train_test_split

# pecah data ke dalam training dan testing
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=0)

# definis K-means
kmeans = MiniBatchKMeans(n_clusters=3, random_state=42).fit(X_train)
X_test_clusters = kmeans.predict(X_test)
X_test_clusters_centers = kmeans.cluster_centers_
dist = [np.linalg.norm(x-y) for x, y in zip(X_test, X_test_clusters_centers[X_test_clusters])]

# membuat prediksi fraud berdasarkan dari outliner
km_y_pred = np.array(dist)
km_y_pred[dist >= np.percentile(dist, 95)] = 1
km_y_pred[dist < np.percentile(dist, 95)] = 0

km_y_pred

np.unique(km_y_pred)

X_test_clusters_centers

np.unique(X_test_clusters)

"""### cek hasil dari model

kita telah mengetahui semua pengamatan sebagai penipuan, jika mereka berada di persentil ke-5 teratas dalam jarak dari pusat
"""

import seaborn as sn
from sklearn.metrics import confusion_matrix, roc_auc_score

print(roc_auc_score(y_test, km_y_pred))

def plot_confusion_matrix(km_cm):
    df_cm = pd.DataFrame(km_cm, ['True Normal','True Fraud'],['Pred Normal','Pred Fraud'])
    plt.figure(figsize = (8,4))
    
    sn.set(font_scale=1.4) 
    sn.heatmap(df_cm, annot=True, annot_kws={"size": 16}, fmt='g')
    
    plt.show()

# Cmembuat konfusion matriks
km_cm = confusion_matrix(y_test, km_y_pred)

# plorting hasil dari confusion matriks
plot_confusion_matrix(km_cm)

"""deengan menurunkan ambang seperti %93 untuk kasus yang ditandai sebagai penipuan, kita menandai lebih banyak kasus secara keseluruhan tetapi dengan demikian juga mendapatkan lebih banyak false positive

### menggunakan metode ``DB scan``
"""

y.shape

pred_labels.shape

from sklearn.cluster import DBSCAN
from sklearn.metrics import homogeneity_score, silhouette_score

db = DBSCAN(eps=.9, min_samples=10, n_jobs=-1).fit(X_scaled) 
pred_labels = db.labels_
n_clusters = len(set(pred_labels)) - (1 if -1 in y else 0)

print('Estimated number of clusters: %d' % n_clusters)
print("Homogeneity: %0.3f" % homogeneity_score(y, pred_labels))
print("Silhouette Coefficient: %0.3f" % silhouette_score(X_scaled, pred_labels))

"""jumlah cluster jauh lebih banyak dibandingkan dengan K-means. Untuk deteksi penipuan ini untuk saat ini OK

### menilai cluster terkecil
"""

pred_labels.shape

# Hitung pengamatan di setiap nomor cluster
counts = np.bincount(pred_labels[pred_labels >= 0])
print(counts)

# urutkan jumlah sampel cluster dan ambil 3 cluster terkecil teratas
smallest_clusters = np.argsort(counts)[:3]
print("The smallest clusters are clusters:")      
print(smallest_clusters)

# cetak jumlah cluster terkecil saja
print("Their counts are:")      
print(counts[smallest_clusters])

"""jadi sekarang kita tahu kluster terkecil mana yang dapat Anda tandai sebagai penipuan. jika kita ingin mengambil lebih banyak kelompok terkecil, kita menyebarkan jaring kit lebih luas dan menangkap lebih banyak penipuan, tetapi kemungkinan besar juga lebih banyak false positive. terserah analis penipuan untuk menemukan jumlah kasus yang tepat untuk ditandai dan diselidiki

### cek hasil
"""

# buat kerangka data dari nomor cluster yang diprediksi dan label penipuan
df = pd.DataFrame({'clusternr':pred_labels,'fraud':y})

# Buat kondisi yang menandai penipuan untuk kluster terkecil
df['predicted_fraud'] = np.where((df['clusternr']==21) | (df['clusternr']==17) | (df['clusternr']==9), 1, 0)

print(pd.crosstab(df.fraud, df.predicted_fraud, rownames=['Actual Fraud'], colnames=['Flagged Fraud']))

"""bagaimana ini dibandingkan dengan model K-means? Hal baiknya adalah: dari semua kasus yang ditandai, kira-kira 2/3 sebenarnya adalah penipuan! Karena kira hanya mengambil tiga kelompok terkecil, menurut definisi kita menandai lebih sedikit kasus penipuan, jadi kita menangkap lebih sedikit tetapi juga memiliki lebih sedikit kesalahan positif. Namun, kita kehilangan cukup banyak kasus penipuan. Meningkatkan jumlah kluster terkecil yang kita tandai dapat meningkatkannya, tentu saja dengan mengorbankan lebih banyak false positive

## deteksi fraud berdasarkan teks

### pencarian kata dengan kerangka data
"""

df = pd.read_csv('enron_emails_clean.csv')
df.head()

df.shape

# Temukan semua email bersih yang berisi 'sell enron stock'
mask = df['clean_content'].str.contains('sell enron stock', na=False)

# Select the data from df using the mask
df.loc[mask]

"""### menggunakan daftar istilah"""

searchfor = ['enron stock', 'sell stock', 'stock bonus', 'sell enron stock']

filtered_emails = (df.loc[df['clean_content'].str.contains('|'.join(searchfor), na=False)])
filtered_emails.shape

"""dengan menggabungkan istilah pencarian dengan tanda 'or', yaitu `|`,kita dapat mencari banyak istilah dalam kumpulan data Anda dengan sangat mudah.

### buat flag

membuat flag dengan fungsi ``np.where()``
"""

df['flag'] = np.where((df['clean_content'].str.contains('|'.join(searchfor)) == True), 1, 0)
count = df['flag'].value_counts()
print(count)

"""### menghapus stopword"""

import string

from nltk.corpus import stopwords

# stopword untuk dikecualikan
stop = set(stopwords.words('english'))
stop.update(("to","cc","subject","http","from","sent", "ect", "u", "fwd", "www", "com"))

exclude = set(string.punctuation)

"""### Cleaning text data"""

df = df[df.clean_content.notnull()]

from nltk.stem.wordnet import WordNetLemmatizer

lemma = WordNetLemmatizer()

def clean(text, stop):
    text = text.strip()
    stop_free = " ".join([word for word in text.lower().split() if ((word not in stop) and (not word.isdigit()))])
    punc_free = ''.join(word for word in stop_free if word not in exclude)
    normalized = " ".join(lemma.lemmatize(word) for word in punc_free.split())      
    return normalized

text_clean=[]
for text in df['clean_content']:
    text_clean.append(clean(text, stop).split())    
print(text_clean)

len(text_clean)

text_clean[:1][0][:10]

"""sekarang kita telah membersihkan data Anda sepenuhnya dengan langkah-langkah yang diperlukan, termasuk memecah teks menjadi kata-kata, menghapus stopwords dan tanda baca, dan lemmatisasi kata-kata Anda.

### buat dict dan corpus
"""

import gensim
from gensim import corpora

dictionary = corpora.Dictionary(text_clean)

corpus = [dictionary.doc2bow(text) for text in text_clean]

print(dictionary)
print(corpus[:2])

"""### menggunakan model LDA

latent Dirichlet Allocation (LDA) adalah model probabilistik generatif dari koleksi data diskrit seperti korpus teks. Ide dasarnya adalah bahwa dokumen direpresentasikan sebagai campuran acak atas topik laten (tidak terlihat).

LDA merupakan model Bayesian hirarki tiga tingkat, di mana setiap item koleksi dimodelkan sebagai campuran terbatas atas serangkaian set topik. Setiap topik dimodelkan sebagai campuran tak terbatas melalui set yang mendasari probabilitas topik. Dalam konteks pembuatan model teks, probabilitas topik memberikan representasi eksplisit dari sebuah dokumen
"""

# Define the LDA model
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=5)

# Save the topics and top 5 words
topics = ldamodel.print_topics(num_words=5)

# Print the results
for topic in topics:
    print(topic)

"""kita sekarang telah berhasil membuat model topik pertama kita pada data email Enron. Namun, cetakan kata-kata tidak benar-benar memberi kita informasi yang cukup untuk menemukan topik yang mungkin mengarahkan kita ke tanda-tanda penipuan. Oleh karena itu, kita harus memeriksa hasil model dengan cermat agar dapat mendeteksi apa pun yang dapat dikaitkan dengan penipuan dalam data kita

### menemukan penipu berdasarkan topik
"""

def get_topic_details(ldamodel, corpus):
    topic_details_df = pd.DataFrame()
    for i, row in enumerate(ldamodel[corpus]):
        row = sorted(row, key=lambda x: (x[1]), reverse=True)
        for j, (topic_num, prop_topic) in enumerate(row):
            if j == 0:  # => topik dominan
                wp = ldamodel.show_topic(topic_num)
                topic_keywords = ", ".join([word for word, prop in wp])
                topic_details_df = topic_details_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)
    topic_details_df.columns = ['Dominant_Topic', '% Score', 'Topic_Keywords']
    return topic_details_df

# jalankan fungsi get_topic_details dan periksa hasilnya
print(get_topic_details(ldamodel, corpus))

# tambahkan teks asli ke detail topik dalam kerangka data
contents = pd.DataFrame({'Original text': text_clean})
topic_details = pd.concat([get_topic_details(ldamodel, corpus), contents], axis=1)
topic_details.head()

topic_details['flag'] = np.where((topic_details['Dominant_Topic'] == 3.0), 1, 0)
print(topic_details.head())
